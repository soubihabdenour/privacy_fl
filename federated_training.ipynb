{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Federated Training of Multiple CNN Architectures on Common Vision Datasets\n",
        "\n",
        "This notebook demonstrates how to train three popular convolutional neural networks\u2014ResNet-18, MobileNetV2, and AlexNet\u2014on three standard datasets (CIFAR-10, CIFAR-100, and MNIST) using a simple federated learning setup powered by [Flower](https://flower.dev/). Each scenario uses a simulated set of federated clients so you can experiment locally without standing up separate devices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Environment Setup\n",
        "\n",
        "Uncomment the following cell to install the required dependencies if they are not already available in your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Uncomment to install dependencies when running in a fresh environment\n",
        "# !pip install torch torchvision flwr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Imports and Utilities\n",
        "\n",
        "We define helper functions for creating datasets, partitioning them across clients, building models with the right output dimensions, and running basic training/evaluation steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, Iterable, List, Tuple\n",
        "\n",
        "import flwr as fl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "@dataclass\n",
        "class DatasetConfig:\n",
        "    name: str\n",
        "    num_classes: int\n",
        "    input_size: Tuple[int, int, int]\n",
        "    train_set: torch.utils.data.Dataset\n",
        "    test_set: torch.utils.data.Dataset\n",
        "\n",
        "\n",
        "def get_transforms(dataset: str) -> Tuple[T.Compose, T.Compose]:\n",
        "    normalize = {\n",
        "        \"cifar10\": ([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),\n",
        "        \"cifar100\": ([0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]),\n",
        "        \"mnist\": ([0.1307], [0.3081]),\n",
        "    }[dataset]\n",
        "\n",
        "    train_transform = T.Compose(\n",
        "        [\n",
        "            T.Resize(32),\n",
        "            T.RandomCrop(32, padding=4),\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(*normalize),\n",
        "        ]\n",
        "    )\n",
        "    test_transform = T.Compose([\n",
        "        T.Resize(32),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(*normalize),\n",
        "    ])\n",
        "    return train_transform, test_transform\n",
        "\n",
        "\n",
        "def load_dataset(dataset: str, root: str = \"./data\") -> DatasetConfig:\n",
        "    dataset = dataset.lower()\n",
        "    train_tf, test_tf = get_transforms(dataset)\n",
        "\n",
        "    if dataset == \"cifar10\":\n",
        "        train_set = torchvision.datasets.CIFAR10(root=root, train=True, download=True, transform=train_tf)\n",
        "        test_set = torchvision.datasets.CIFAR10(root=root, train=False, download=True, transform=test_tf)\n",
        "        num_classes = 10\n",
        "        input_size = (3, 32, 32)\n",
        "    elif dataset == \"cifar100\":\n",
        "        train_set = torchvision.datasets.CIFAR100(root=root, train=True, download=True, transform=train_tf)\n",
        "        test_set = torchvision.datasets.CIFAR100(root=root, train=False, download=True, transform=test_tf)\n",
        "        num_classes = 100\n",
        "        input_size = (3, 32, 32)\n",
        "    elif dataset == \"mnist\":\n",
        "        train_set = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=train_tf)\n",
        "        test_set = torchvision.datasets.MNIST(root=root, train=False, download=True, transform=test_tf)\n",
        "        num_classes = 10\n",
        "        input_size = (1, 32, 32)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported dataset: {dataset}\")\n",
        "\n",
        "    return DatasetConfig(dataset, num_classes, input_size, train_set, test_set)\n",
        "\n",
        "\n",
        "def partition_dataset(dataset: torch.utils.data.Dataset, num_clients: int) -> List[Subset]:\n",
        "    partition_size = math.ceil(len(dataset) / num_clients)\n",
        "    subsets = []\n",
        "    for i in range(num_clients):\n",
        "        start = i * partition_size\n",
        "        end = min((i + 1) * partition_size, len(dataset))\n",
        "        if start >= len(dataset):\n",
        "            break\n",
        "        indices = list(range(start, end))\n",
        "        subsets.append(Subset(dataset, indices))\n",
        "    return subsets\n",
        "\n",
        "\n",
        "def build_model(name: str, num_classes: int) -> nn.Module:\n",
        "    name = name.lower()\n",
        "    if name == \"resnet18\":\n",
        "        model = torchvision.models.resnet18(weights=None)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    elif name == \"mobilenet_v2\":\n",
        "        model = torchvision.models.mobilenet_v2(weights=None)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "    elif name == \"alexnet\":\n",
        "        model = torchvision.models.alexnet(weights=None)\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {name}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module, optimizer: optim.Optimizer) -> float:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    loss_sum = 0.0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss_sum += loss.item() * images.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "    return loss_sum / len(loader.dataset), correct / len(loader.dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Federated Client Definition\n",
        "\n",
        "Each client trains locally on its partition of the dataset and reports weights and metrics back to the server.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class FederatedClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model_fn: Callable[[], nn.Module], train_subset: Subset, test_subset: Subset, epochs: int, batch_size: int):\n",
        "        self.model_fn = model_fn\n",
        "        self.model = self.model_fn().to(DEVICE)\n",
        "        self.train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        self.test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "        self.epochs = epochs\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    def get_parameters(self, config=None):\n",
        "        return [val.cpu().numpy() for val in self.model.state_dict().values()]\n",
        "\n",
        "    def set_parameters(self, parameters: Iterable):\n",
        "        state_dict = self.model.state_dict()\n",
        "        for key, val in zip(state_dict.keys(), parameters):\n",
        "            state_dict[key] = torch.tensor(val)\n",
        "        self.model.load_state_dict(state_dict)\n",
        "\n",
        "    def fit(self, parameters, config=None):\n",
        "        self.set_parameters(parameters)\n",
        "        for _ in range(self.epochs):\n",
        "            train_one_epoch(self.model, self.train_loader, self.criterion, self.optimizer)\n",
        "        return self.get_parameters(), len(self.train_loader.dataset), {}\n",
        "\n",
        "    def evaluate(self, parameters, config=None):\n",
        "        self.set_parameters(parameters)\n",
        "        loss, accuracy = evaluate(self.model, self.test_loader, self.criterion)\n",
        "        return float(loss), len(self.test_loader.dataset), {\"accuracy\": float(accuracy)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Federated Training Helper\n",
        "\n",
        "The `run_federated_experiment` function configures a dataset, partitions it among simulated clients, and launches a Flower simulation using FedAvg. You can swap in any of the supported datasets and models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def run_federated_experiment(\n",
        "    dataset_name: str,\n",
        "    model_name: str,\n",
        "    num_clients: int = 5,\n",
        "    rounds: int = 3,\n",
        "    local_epochs: int = 1,\n",
        "    batch_size: int = 64,\n",
        "):\n",
        "    cfg = load_dataset(dataset_name)\n",
        "    train_subsets = partition_dataset(cfg.train_set, num_clients)\n",
        "    test_subsets = partition_dataset(cfg.test_set, num_clients)\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        idx = int(cid)\n",
        "        return FederatedClient(\n",
        "            model_fn=lambda: build_model(model_name, cfg.num_classes),\n",
        "            train_subset=train_subsets[idx],\n",
        "            test_subset=test_subsets[idx % len(test_subsets)],\n",
        "            epochs=local_epochs,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "\n",
        "    strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1.0,\n",
        "        min_fit_clients=len(train_subsets),\n",
        "        min_evaluate_clients=len(test_subsets),\n",
        "        min_available_clients=len(train_subsets),\n",
        "        evaluate_metrics_aggregation_fn=lambda metrics: sum(m[\"accuracy\"] * n for _, n, m in metrics) / sum(n for _, n, _ in metrics),\n",
        "    )\n",
        "\n",
        "    print(f\"Starting federated run: dataset={dataset_name}, model={model_name}, clients={len(train_subsets)}\")\n",
        "    history = fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=len(train_subsets),\n",
        "        config=fl.server.ServerConfig(num_rounds=rounds),\n",
        "        strategy=strategy,\n",
        "    )\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Running Experiments\n",
        "\n",
        "The following examples configure short federated runs to keep execution time reasonable. Increase `rounds` or `local_epochs` for deeper training when resources allow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Example: ResNet-18 on CIFAR-10\n",
        "# history_resnet_cifar10 = run_federated_experiment(\"cifar10\", \"resnet18\", num_clients=5, rounds=3, local_epochs=1)\n",
        "\n",
        "# Example: MobileNetV2 on CIFAR-100\n",
        "# history_mobilenet_cifar100 = run_federated_experiment(\"cifar100\", \"mobilenet_v2\", num_clients=5, rounds=3, local_epochs=1)\n",
        "\n",
        "# Example: AlexNet on MNIST\n",
        "# history_alexnet_mnist = run_federated_experiment(\"mnist\", \"alexnet\", num_clients=3, rounds=3, local_epochs=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Visualizing Accuracy Trends\n",
        "\n",
        "If you run multiple experiments, you can collect their histories and plot global accuracy across rounds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_history(history, label: str):\n",
        "    rounds = list(range(1, len(history.metrics_distributed)[\"accuracy\"] + 1))\n",
        "    accuracies = [entry[1] for entry in history.metrics_distributed[\"accuracy\"]]\n",
        "    plt.plot(rounds, accuracies, marker=\"o\", label=label)\n",
        "    plt.xlabel(\"Round\")\n",
        "    plt.ylabel(\"Global accuracy\")\n",
        "    plt.title(\"Federated Accuracy over Rounds\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage after running experiments:\n",
        "# plot_history(history_resnet_cifar10, \"ResNet18 CIFAR-10\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}